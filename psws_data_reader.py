import os
import sys
import digital_rf as drf
import pandas as pd
import math
import numpy as np
from tqdm import tqdm
import datetime
import pytz
import pickle
import logging
from typing import Optional, Dict, Any, List
from scipy import signal

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s", handlers=[logging.StreamHandler(sys.stdout)])

class PSWSDataReader:
    """ Class for reading and processing Digital RF (DRF) data generated by Grape2 PSWS stations."""
    
    def __init__(self, datadir: str, cachedir: Optional[str] = None, resampled_fs: int = 2000, batch_size_mins: int = 30):
        self.datadir = datadir
        self.cachedir = cachedir
        self.resampled_fs = resampled_fs
        self.batch_size_mins = batch_size_mins
        
        if cachedir:
            self._ensure_directory_exists(cachedir)

        self.dro, self.dmr = self._initialize_readers()
        self.fs = int(self.dmr.get_samples_per_second())
        self.start_index, self.end_index = self.dro.get_bounds("ch0")
        self.rf_dict = self.dro.get_properties("ch0", sample=self.start_index)
        self.utc_date = self._get_initial_date()

        self.station, self.node, self.center_frequencies = self._extract_metadata()
    
    def _ensure_directory_exists(self, directory: str):
        """Ensure that the cache directory exists."""
        if not os.path.exists(directory):
            os.makedirs(directory)

    def _initialize_readers(self):
        """Initialize Digital RF and Metadata readers."""
        metadir = os.path.join(self.datadir, "ch0", "metadata")
        return drf.DigitalRFReader(self.datadir), drf.DigitalMetadataReader(metadir)
    
    def _get_initial_date(self) -> datetime.date:
        """Retrieve the initial UTC timestamp and return it as a date."""
        rf_properties = self.dro.get_properties("ch0", sample=self.start_index)
        init_timestamp = rf_properties["init_utc_timestamp"]
        return datetime.datetime.fromtimestamp(init_timestamp, tz=pytz.utc).date()
    
    def _extract_metadata(self) -> tuple[str, str, List[float]]:
        """Extract metadata such as station, node, and center frequencies."""
        latest_meta = self.dmr.read_latest()
        latest_inx = list(latest_meta.keys())[0]
        station = latest_meta[latest_inx]["callsign"]
        node = latest_meta[latest_inx]["station_node_number"]
        center_frequencies = latest_meta[latest_inx]["center_frequencies"]
        return station, node, center_frequencies
    
    def _get_cache_file_path(self, channel: int, resampled: bool = False) -> str:
        """Generate the file path for the cached data."""
        suffix = f"{self.resampled_fs}Hz_" if resampled else ""
        return os.path.join(
            self.cachedir,
            f"{self.utc_date}_{self.node}_RAWDATA_{suffix}{channel}.ba.pkl"
        )

    def read_data(self, channel: int) -> np.ndarray:
        """Read and return the DRF data for the specified channel."""
        logging.info(f"Reading DRF data for channel {channel} (batch size = {self.batch_size_mins} mins)")

        if self.cachedir:
            raw_cache_path = self._get_cache_file_path(channel)
            if os.path.exists(raw_cache_path):
                return self._load_cached_data(raw_cache_path, channel)
            else:
                data = self._read_and_cache_data(channel, raw_cache_path)
        else:
            data = self._read_data_in_batches(channel)

        logging.info(f"Loaded data for channel {channel} successfully!")
        return data
    
    def _read_and_cache_data(self, channel: int, cache_path: str) -> np.ndarray:
        """Read data in batches, cache it, and return the result."""
        data = self._read_data_in_batches(channel)
        self._cache_data(cache_path, data)
        return data
    
    def _load_cached_data(self, cache_path: str, channel: int) -> np.ndarray:
        """Load data from cache or resample it if needed."""
        resampled_path = self._get_cache_file_path(channel, resampled=True)

        if os.path.exists(resampled_path):
            logging.info(f"Using cached resampled file: {resampled_path}")
            return self._load_pickle(resampled_path)

        logging.info(f"Using cached file: {cache_path}")
        data = self._load_pickle(cache_path)
        return self._resample_and_cache(data, resampled_path)

    def _read_data_in_batches(self, channel: int) -> np.ndarray:
        """Read data from DRF in batches."""
        cont_data_arr = self.dro.get_continuous_blocks(self.start_index, self.end_index, "ch0")
        batch_size_samples = self.fs * 60 * self.batch_size_mins
        read_iters = math.ceil((self.end_index - self.start_index) / batch_size_samples)

        batches = []
        start_sample = list(cont_data_arr.keys())[0]
        for _ in tqdm(range(read_iters), desc="Reading Batches"):
            batch = self.dro.read_vector(start_sample, batch_size_samples, "ch0", channel)
            batches.append(batch)
            start_sample += batch_size_samples

        return np.concatenate(batches)

    def _resample_and_cache(self, data: np.ndarray, cache_path: str) -> np.ndarray:
        """Resample the data and cache the result."""
        decimation_factor = int(self.fs / self.resampled_fs)
        resampled_signal = signal.decimate(data, decimation_factor, ftype="fir", zero_phase=True)
        self._cache_data(cache_path, resampled_signal)
        logging.info(f"Resampled data shape: {resampled_signal.shape}")
        return resampled_signal

    def _cache_data(self, path: str, data: np.ndarray):
        """Cache the data to a pickle file."""
        with open(path, "wb") as fl:
            pickle.dump(data, fl)
        logging.info(f"Cached data at: {path}")

    def _load_pickle(self, path: str) -> np.ndarray:
        """Load data from a pickle file."""
        with open(path, "rb") as fl:
            return pickle.load(fl)

    def get_metadata(self) -> Dict[str, Any]:
        """Return metadata information."""
        return {
            "sampling_rate": self.fs,
            "center_frequencies": self.center_frequencies,
            "station": self.station,
            "utc_date": self.utc_date,
        }

if __name__ == "__main__":
    data_dir = sys.argv[1]
    cache_dir = 'cache'
    data_reader = PSWSDataReader(data_dir, cachedir=cache_dir, resampled_fs=2000) 
    for i in range(3): 
        data_reader.read_data(i)